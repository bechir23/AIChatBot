{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "from bs4 import BeautifulSoup\n",
    "from email import message_from_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace,Experiment,Run\n",
    "\n",
    "ws = Workspace.from_config(path='./config.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:/Users/bechi/Downloads/archive/emails.csv\", nrows=70000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['message'][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from email import message_from_string\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_email(email_str):\n",
    "    msg = message_from_string(email_str)\n",
    "    \n",
    "    # Extract basic email information\n",
    "    from_address = msg.get('From')\n",
    "    to_address = msg.get('To')\n",
    "    subject = msg.get('Subject')\n",
    "    date = msg.get('Date')\n",
    "    \n",
    "    body = \"\"\n",
    "    \n",
    "    if msg.is_multipart():\n",
    "        for part in msg.walk():\n",
    "            content_type = part.get_content_type()\n",
    "            content_disposition = str(part.get(\"Content-Disposition\"))\n",
    "\n",
    "            if \"attachment\" in content_disposition:\n",
    "                continue\n",
    "            \n",
    "            if content_type == 'text/plain':\n",
    "                body += part.get_payload(decode=True).decode(errors='ignore')\n",
    "            elif content_type == 'text/html':\n",
    "                html_content = part.get_payload(decode=True).decode(errors='ignore')\n",
    "                soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "                body += soup.get_text(separator=' ')\n",
    "    else:\n",
    "        body = msg.get_payload(decode=True).decode(errors='ignore')\n",
    "    \n",
    "    body = re.sub(r'\\b(see|attached|file|attachment|https?://\\S+)\\b', '', body, flags=re.IGNORECASE)\n",
    "    body = re.sub(r'\\b[a-zA-Z]{1,2}\\b', '', body)  \n",
    "    body = re.sub(r'(.)\\1{4,}', '', body)  \n",
    "    body = re.sub(r'[a-zA-Z]{30,}', '', body)  \n",
    "  \n",
    "   \n",
    "\n",
    "    \n",
    "    return from_address, to_address, subject, date, body\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_str = \"\"\"Your raw email string here\"\"\"\n",
    "from_address, to_address, subject, date, body = clean_email(email_str)\n",
    "print(\"From:\", from_address)\n",
    "print(\"To:\", to_address)\n",
    "print(\"Subject:\", subject)\n",
    "print(\"Date:\", date)\n",
    "print(\"Body:\", body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.DataFrame([clean_email(data[\"message\"][i]) for i in range(len(data))], \n",
    "                            columns=['from_address', 'to_address', 'subject', 'date', 'body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = cleaned_data.dropna()\n",
    "cleaned_data=cleaned_data[cleaned_data[\"body\"]!=\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_rows = cleaned_data.duplicated()\n",
    "print(duplicated_rows.sum())\n",
    "cleaned_data = cleaned_data.drop_duplicates()\n",
    "len(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def extract_email(email_str):\n",
    "    left = ''\n",
    "    right = ''\n",
    "    email = '@'\n",
    "    \n",
    "    if '@' in email_str:\n",
    "        left, right = email_str.split('@', 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in right:\n",
    "            if i.isalnum() or (i == '.' and right[0] != '.'):\n",
    "                email += i\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        for i in left[::-1]:\n",
    "            if i.isalnum() or (i == '.' and left[-1] != '.'):\n",
    "                email = i + email\n",
    "            else:\n",
    "                break      \n",
    "    \n",
    "    return email\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['from_address'] = cleaned_data['from_address'].apply(extract_email)\n",
    "cleaned_data['to_address'] = cleaned_data['to_address'].apply(extract_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_data[(cleaned_data['from_address'] == '@') | (cleaned_data['to_address'] == '@')])\n",
    "cleaned_data = cleaned_data[(cleaned_data['from_address'] != '@') & (cleaned_data['to_address'] != '@')]\n",
    "cleaned_data = cleaned_data[~(cleaned_data['from_address'].str.startswith('.') | cleaned_data['from_address'].str.endswith('.'))]\n",
    "cleaned_data=cleaned_data[~(cleaned_data['to_address'].str.startswith('.') | cleaned_data['to_address'].str.endswith('.'))]\n",
    "print(len(cleaned_data))\n",
    "cleaned_data[\"date\"][0].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cleaned_data[cleaned_data['date'].str[3] == ',']))\n",
    "cleaned_data = cleaned_data[cleaned_data['date'].str[3] == ',']\n",
    "print(len(cleaned_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def months():\n",
    "    month=[]\n",
    "    a=[]\n",
    "    count=0\n",
    "    for date in cleaned_data['date']:\n",
    "        day, subdate = date.split(',')\n",
    "        if day=='':count += count\n",
    "        subdate = subdate.split(' ')\n",
    "        if(len(subdate)>7) |(len(subdate)<7):count += count\n",
    "        if (len(subdate)==7): \n",
    "             if not subdate[2].isalpha():\n",
    "               a.append(date)\n",
    "             if(subdate[2]) not in month:\n",
    "              month.append(subdate[2])\n",
    "    print(month)  \n",
    "    print(count)  \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=months()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = ['re:', 'RE:', 'rE:']\n",
    "\n",
    "filtered_data = cleaned_data[cleaned_data['subject'].apply(lambda x: any(x.startswith(prefix) for prefix in prefixes))]\n",
    "print(len(filtered_data))\n",
    "print(len(cleaned_data[cleaned_data['subject'].str.startswith('Re:')]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_subject(subject):\n",
    "    prefixes = ['re:', 'RE:', 'rE:']\n",
    "\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        if prefix in subject:\n",
    "            subject = subject.replace(prefix, 'Re:', 1)\n",
    "            \n",
    "    prefixes = ['Fw:']\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        if subject.startswith(prefix):\n",
    "            subject = subject.replace(prefix, 'FW:', 1)\n",
    "            break\n",
    "    return subject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = ['fWD:',  'FwD:', 'fWd:', 'FWd:', 'FWD:', 'fwD:','fwd:','fW']\n",
    "cleaned_data = cleaned_data[~cleaned_data['subject'].apply(lambda x: any(x.startswith(prefix) for prefix in prefixes))]\n",
    "print(len(cleaned_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['subject'] = cleaned_data['subject'].apply(clean_subject)\n",
    "len(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = ['fWD:',  'FwD:', 'fWd:', 'FWd:', 'FWD:', 'fwD:','fwd:','fW']\n",
    "\n",
    "filtered_data = cleaned_data[cleaned_data['subject'].apply(lambda x: any(prefix in x for prefix in prefixes)) ]\n",
    "print(len(filtered_data['subject']))\n",
    "cleaned_data = cleaned_data[~cleaned_data['subject'].apply(lambda x: any(prefix in x for prefix in prefixes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.duplicated().sum()\n",
    "print(len(cleaned_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = cleaned_data.drop_duplicates()\n",
    "print(len(cleaned_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['body'] = cleaned_data['body'].apply(normalize_text)\n",
    "cleaned_data['subject'] = cleaned_data['subject'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['date'] = cleaned_data['date'].str.replace(r\"\\s\\([A-Z]{3}\\)\", \"\")\n",
    "cleaned_data[\"date\"]= pd.to_datetime(cleaned_data['date'], utc=True)\n",
    "print(cleaned_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['year'] = cleaned_data['date'].dt.year \n",
    "cleaned_data['month'] = cleaned_data['date'].dt.month\n",
    "cleaned_data['day'] = cleaned_data['date'].dt.day\n",
    "cleaned_data['hour'] = cleaned_data['date'].dt.hour\n",
    "cleaned_data['minute'] = cleaned_data['date'].dt.minute\n",
    "cleaned_data['second'] = cleaned_data['date'].dt.second\n",
    "cleaned_data['day_of_week'] = cleaned_data['date'].dt.dayofweek\n",
    "cleaned_data['day_name'] = cleaned_data['date'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_email_body(body_text):\n",
    "    cleaned_body=body_text\n",
    "    if isinstance(body_text, str):\n",
    "        cleaned_body = re.sub(r'\\b\\d{1,2}:\\d{2}\\b', '', cleaned_body)  \n",
    "        cleaned_body = re.sub(r'\\bmultipart\\b', '', cleaned_body, flags=re.IGNORECASE)  \n",
    "        cleaned_body = re.sub(r'\\s+', ' ', cleaned_body)  \n",
    "        cleaned_body = re.sub(r'https?://\\S+', '', cleaned_body)  \n",
    "        cleaned_body = re.sub(r'\\b\\w{1,2}\\b', '', cleaned_body)  \n",
    "        cleaned_body = re.sub(r'\\s+', ' ', cleaned_body)  \n",
    "        cleaned_body = re.sub(r'\\d+', '', cleaned_body)\n",
    "\n",
    "        words = cleaned_body.split()\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        cleaned_body = ' '.join(words)\n",
    "\n",
    "      \n",
    "        return cleaned_body.strip()  \n",
    "    else:\n",
    "        return body_text\n",
    "\n",
    "def clean_text(text):\n",
    "            return re.sub(r'\\b\\w{1,2}\\b', '', text).strip()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['body'] = cleaned_data['body'].apply(clean_email_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['text'] = cleaned_data['subject'] + ' ' + cleaned_data['body']   \n",
    "cleaned_data['text'] = cleaned_data['text'].apply(clean_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.head()\n",
    "cleaned_data.duplicated().sum()\n",
    "cleaned_data.drop_duplicates(inplace=True)\n",
    "cleaned_data.duplicated().sum()\n",
    "cleaned_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#from nltk.corpus import wordnet\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#import spacy\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "# verify that words are valid English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_valid_words(texts, threshold=0.5):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    word_tfidf_sums = X.sum(axis=0).A1  \n",
    "    \n",
    "    word_tfidf_sum_dict = dict(zip(feature_names, word_tfidf_sums))\n",
    "    \n",
    "    valid_words = {word for word, score_sum in word_tfidf_sum_dict.items() if score_sum > threshold}\n",
    "    \n",
    "    return valid_words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_body_with_valid_words(texts, valid_words):\n",
    "    cleaned_texts = []\n",
    "    \n",
    "    for body in texts:\n",
    "        words = [word for word in body.split() if word in valid_words]\n",
    "        cleaned_texts.append(' '.join(words))\n",
    "    \n",
    "    return cleaned_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_words = get_valid_words(cleaned_data['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleaned_data['text'] = clean_body_with_valid_words(cleaned_data['text'], valid_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def identify_abbreviations(emails):\n",
    "    all_words = ' '.join(emails).split() \n",
    "    word_counter = Counter(all_words) \n",
    "    \n",
    "    print(word_counter.most_common(100))\n",
    "\n",
    "identify_abbreviations(cleaned_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "Y=label_encoder.fit_transform(cleaned_data['from_address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X=vectorizer.fit_transform(cleaned_data['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(np.float32).tocsc()\n",
    "x_test = x_test.astype(np.float32).tocsc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "model = SGDClassifier(verbose=1, n_jobs=-1)\n",
    "model.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "joblib.dump(model, 'model.pkl')\n",
    "joblib.dump(vectorizer,'vectorizer.pkl')\n",
    "joblib.dump(label_encoder,'label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy\",model.score(x_test,y_test))\n",
    "print(\"recall\",recall_score(y_test,y_pred,average='weighted'))\n",
    "print(\"precision\",precision_score(y_test,y_pred,average='weighted'))    \n",
    "print(\"f1_score\",f1_score(y_test,y_pred,average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of features in the vectorizer: {len(vectorizer.get_feature_names_out())}\")\n",
    "print(f\"Model's expected number of features: {model.n_features_in_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sender(query, model, vectorizer, label_encoder):\n",
    "    query = clean_text(query)\n",
    "    query = clean_email_body(query)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    query_vector = query_vector.astype(np.float32).tocsc()\n",
    "    prediction = model.predict(query_vector)\n",
    "    predicted_label = label_encoder.inverse_transform(prediction)\n",
    "    return predicted_label[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sender('traveling business meeting takes fun trip especially prepare presentation would suggest holding business plan meetings take trip without formal business meetings would even try get honest opinions whether trip even desired necessary far business meetings think would productive try stimulate discussions across different groups working often presenter speaks others quiet waiting turn meetings might better held round table discussion format suggestion austin play golf rent ski boat jet ski flying somewhere takes much tim', model, vectorizer, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data[(cleaned_data['from_address']=='sally.beck@enron.com')&(cleaned_data['date']==\"2001-01-11 17:59:00+00:00\")][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l='traveling business meeting takes fun trip especially prepare presentation would suggest holding business plan meetings take trip'.split(' ')\n",
    "for index, row in cleaned_data[cleaned_data['from_address']=='phillip.allen@enron.com'].iterrows():\n",
    "    if sum(word in row['text'] for word in l) >20:\n",
    "        print(row['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
